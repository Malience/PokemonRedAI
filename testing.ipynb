{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.set_default_device('cuda') \n",
    "\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "sess_id = str(uuid.uuid4())[:8]\n",
    "sess_path = Path(f'session_{sess_id}')\n",
    "sess_path.mkdir(exist_ok=True)\n",
    "\n",
    "states_path = Path('states')\n",
    "states_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emulator import Emulator, SIMPLE_ACTION_SPACE, MOVEMENT_ACTION_SPACE\n",
    "from pokered_vecenv import PokeRedVecEnv\n",
    "\n",
    "from explore_low_agent import ExploreLowAgent\n",
    "from basic_flee_agent import BasicFleeAgent\n",
    "\n",
    "from policy import Policy\n",
    "\n",
    "gb_path = './PokemonRed.gb'\n",
    "init_state = './has_pokedex_nballs.state'\n",
    "#init_state = 'states/11_5_0-0.state'\n",
    "\n",
    "emulators = [Emulator(sess_path, gb_path, instance_id=f'main_{i}', headless=True) for i in range(10)]\n",
    "vec_env = PokeRedVecEnv(emulators, [init_state])\n",
    "\n",
    "flee_policy = Policy([], SIMPLE_ACTION_SPACE)\n",
    "flee_agent = BasicFleeAgent('flee_agent', SIMPLE_ACTION_SPACE)\n",
    "\n",
    "explore_policy = Policy([], MOVEMENT_ACTION_SPACE)\n",
    "explore_agent = ExploreLowAgent('explore_agent', MOVEMENT_ACTION_SPACE, 12)\n",
    "\n",
    "vec_env.register_agent(flee_agent)\n",
    "vec_env.register_agent(explore_agent)\n",
    "vec_env.initial_agent('explore_agent')\n",
    "\n",
    "policies = {'explore_agent': explore_policy, 'flee_agent': flee_policy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Anaconda3\\envs\\pokey\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ppo import PPOSettings, PPOTrainer\n",
    "pposettings = PPOSettings()\n",
    "ppo = PPOTrainer(policies['explore_agent'], pposettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout - explore_agent - Reward = 0.3100000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.22000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.24800000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.26000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2800000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4380000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.20000000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3100000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5880000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2780000000000001, Success = False\n",
      "Time spent stepping: 13.895978689193726\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.011207297444343567\n",
      "policy_loss: 0.008226786740124226\n",
      "entropy: 1.2652487754821777\n",
      "old_approx_kl: 0.03174450248479843\n",
      "approx_kl: 0.02304500713944435\n",
      "Rollout - explore_agent - Reward = 0.25000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.23000000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.25000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.26000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.44800000000000023, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2880000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.19000000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.34800000000000014, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3080000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2880000000000001, Success = False\n",
      "Time spent stepping: 13.126057147979736\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0020768071990460157\n",
      "policy_loss: -0.024077577516436577\n",
      "entropy: 1.2654794454574585\n",
      "old_approx_kl: 0.007758349180221558\n",
      "approx_kl: 0.010836709290742874\n",
      "Rollout - explore_agent - Reward = 0.2800000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3100000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4260000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.26000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2700000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3100000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2800000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.44600000000000023, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4100000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.26000000000000006, Success = False\n",
      "Time spent stepping: 13.779178857803345\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0023154038935899734\n",
      "policy_loss: -0.03143799677491188\n",
      "entropy: 1.2697808742523193\n",
      "old_approx_kl: 0.025263609364628792\n",
      "approx_kl: 0.011846458539366722\n",
      "Rollout - explore_agent - Reward = 0.5600000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.46000000000000024, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4990000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3300000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.35000000000000014, Success = False\n",
      "Rollout - explore_agent - Reward = 0.34400000000000014, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2900000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2980000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2900000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2700000000000001, Success = False\n",
      "Time spent stepping: 14.017398834228516\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003106232499703765\n",
      "policy_loss: -0.03082391619682312\n",
      "entropy: 1.2850598096847534\n",
      "old_approx_kl: 0.012493060901761055\n",
      "approx_kl: 0.010145632550120354\n",
      "Rollout - explore_agent - Reward = 0.25000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.37000000000000016, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4400000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.36000000000000015, Success = False\n",
      "Rollout - explore_agent - Reward = 0.45900000000000024, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4000000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.48000000000000026, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5300000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3000000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3900000000000002, Success = False\n",
      "Time spent stepping: 13.926693677902222\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0032990716863423586\n",
      "policy_loss: -0.02182810753583908\n",
      "entropy: 1.289501667022705\n",
      "old_approx_kl: 0.020758792757987976\n",
      "approx_kl: 0.012847238220274448\n",
      "Rollout - explore_agent - Reward = 0.6380000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6080000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.44600000000000023, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3200000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.47000000000000025, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5680000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4980000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.36000000000000015, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6100000000000003, Success = False\n",
      "Time spent stepping: 13.979559898376465\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.004561297595500946\n",
      "policy_loss: -0.03072274476289749\n",
      "entropy: 1.3095767498016357\n",
      "old_approx_kl: 0.010946675203740597\n",
      "approx_kl: 0.012367810122668743\n",
      "Rollout - explore_agent - Reward = 0.4200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.46000000000000024, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5640000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4320000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5060000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5400000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.46000000000000024, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5800000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7140000000000004, Success = False\n",
      "Time spent stepping: 14.242587566375732\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0035771180409938097\n",
      "policy_loss: -0.03923177719116211\n",
      "entropy: 1.2992362976074219\n",
      "old_approx_kl: 0.012944766320288181\n",
      "approx_kl: 0.011795160360634327\n",
      "Rollout - explore_agent - Reward = 0.49000000000000027, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6400000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7040000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5580000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7360000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.37000000000000016, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5500000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8820000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6110000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6600000000000004, Success = False\n",
      "Time spent stepping: 14.803906679153442\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.00311537878587842\n",
      "policy_loss: -0.04848766326904297\n",
      "entropy: 1.3034402132034302\n",
      "old_approx_kl: 0.011595514602959156\n",
      "approx_kl: 0.011344599537551403\n",
      "Rollout - explore_agent - Reward = 0.3900000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6140000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5700000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1360000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5650000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5100000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8040000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6980000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5880000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5300000000000002, Success = False\n",
      "Time spent stepping: 14.81599736213684\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003975505940616131\n",
      "policy_loss: -0.039030950516462326\n",
      "entropy: 1.2974215745925903\n",
      "old_approx_kl: -0.0007655384833924472\n",
      "approx_kl: 0.011042969301342964\n",
      "Rollout - explore_agent - Reward = 0.7300000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8800000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8780000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5910000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5400000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.3900000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7080000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4000000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5870000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6500000000000004, Success = False\n",
      "Time spent stepping: 15.375658750534058\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0033628172241151333\n",
      "policy_loss: -0.04916141927242279\n",
      "entropy: 1.2720847129821777\n",
      "old_approx_kl: 0.007684836629778147\n",
      "approx_kl: 0.01290757954120636\n",
      "Rollout - explore_agent - Reward = 0.5390000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5890000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8480000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2790000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9160000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.2890000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6650000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0300000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5700000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5990000000000003, Success = False\n",
      "Time spent stepping: 17.686466455459595\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.005631716456264257\n",
      "policy_loss: -0.052843544632196426\n",
      "entropy: 1.2972519397735596\n",
      "old_approx_kl: 0.010416736826300621\n",
      "approx_kl: 0.015066622756421566\n",
      "Rollout - explore_agent - Reward = 0.6180000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.48000000000000026, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6680000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5100000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4400000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.48000000000000026, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5970000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8880000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4050000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.48100000000000026, Success = False\n",
      "Time spent stepping: 16.206058740615845\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003319543320685625\n",
      "policy_loss: -0.04199563339352608\n",
      "entropy: 1.2770347595214844\n",
      "old_approx_kl: 0.010928761214017868\n",
      "approx_kl: 0.012269147671759129\n",
      "Rollout - explore_agent - Reward = 0.47000000000000025, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5930000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9600000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5890000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7960000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8840000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6580000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6690000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5750000000000003, Success = False\n",
      "Time spent stepping: 16.710484266281128\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.002789795631542802\n",
      "policy_loss: -0.04937770590186119\n",
      "entropy: 1.2581133842468262\n",
      "old_approx_kl: 0.014390203170478344\n",
      "approx_kl: 0.01337649766355753\n",
      "Rollout - explore_agent - Reward = 0.7000000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4050000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6580000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6560000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6100000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8380000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5980000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4300000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6880000000000004, Success = False\n",
      "Time spent stepping: 15.566815376281738\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0030749530997127295\n",
      "policy_loss: -0.046055033802986145\n",
      "entropy: 1.2313811779022217\n",
      "old_approx_kl: 0.01597871631383896\n",
      "approx_kl: 0.01158747635781765\n",
      "Rollout - explore_agent - Reward = 0.8440000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.4300000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6280000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.46700000000000025, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9500000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5930000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7300000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7870000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7160000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5160000000000002, Success = False\n",
      "Time spent stepping: 14.473331212997437\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.004159820731729269\n",
      "policy_loss: -0.04714591056108475\n",
      "entropy: 1.1993963718414307\n",
      "old_approx_kl: 0.006074448116123676\n",
      "approx_kl: 0.014659322798252106\n",
      "Rollout - explore_agent - Reward = 0.7780000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7800000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0200000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7930000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6690000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1260000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.34900000000000014, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7190000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7700000000000005, Success = False\n",
      "Time spent stepping: 14.534162759780884\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0034869140945374966\n",
      "policy_loss: -0.040050260722637177\n",
      "entropy: 1.2134541273117065\n",
      "old_approx_kl: 0.018359512090682983\n",
      "approx_kl: 0.012594684958457947\n",
      "Rollout - explore_agent - Reward = 1.0460000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6450000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7980000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5980000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5200000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8300000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7200000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.2200000000000009, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6240000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6680000000000004, Success = False\n",
      "Time spent stepping: 14.741087198257446\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0035114274360239506\n",
      "policy_loss: -0.050168078392744064\n",
      "entropy: 1.1927498579025269\n",
      "old_approx_kl: 0.011909606866538525\n",
      "approx_kl: 0.013998388312757015\n",
      "Rollout - explore_agent - Reward = 1.360000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5190000000000002, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0090000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5970000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 1.275000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9500000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7780000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7380000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1700000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8860000000000006, Success = False\n",
      "Time spent stepping: 14.602990627288818\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003366295713931322\n",
      "policy_loss: -0.04498567059636116\n",
      "entropy: 1.2030280828475952\n",
      "old_approx_kl: 0.02230716310441494\n",
      "approx_kl: 0.013447394594550133\n",
      "Rollout - explore_agent - Reward = 0.6490000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1260000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0240000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7500000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1100000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9000000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7500000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7600000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6500000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8100000000000005, Success = False\n",
      "Time spent stepping: 14.274125814437866\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003305999794974923\n",
      "policy_loss: -0.045133233070373535\n",
      "entropy: 1.1502315998077393\n",
      "old_approx_kl: 0.016307229176163673\n",
      "approx_kl: 0.010946100577712059\n",
      "Rollout - explore_agent - Reward = 0.6290000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7880000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0480000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8160000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0000000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.49000000000000027, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5600000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0500000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0700000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7300000000000004, Success = False\n",
      "Time spent stepping: 14.366024255752563\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003124723443761468\n",
      "policy_loss: -0.048120636492967606\n",
      "entropy: 1.141215205192566\n",
      "old_approx_kl: 0.021946925669908524\n",
      "approx_kl: 0.015148570761084557\n",
      "Rollout - explore_agent - Reward = 1.187000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8550000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7100000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8560000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7200000000000004, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5970000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8080000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.6100000000000003, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9500000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 0.5000000000000002, Success = False\n",
      "Time spent stepping: 14.459079027175903\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.0031275004148483276\n",
      "policy_loss: -0.051535721868276596\n",
      "entropy: 1.1472058296203613\n",
      "old_approx_kl: 0.008866650052368641\n",
      "approx_kl: 0.016065437346696854\n",
      "Rollout - explore_agent - Reward = 1.2400000000000009, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1500000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 0.8400000000000005, Success = False\n",
      "Rollout - explore_agent - Reward = 0.44800000000000023, Success = False\n",
      "Rollout - explore_agent - Reward = 0.9870000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0150000000000006, Success = False\n",
      "Rollout - explore_agent - Reward = 1.1000000000000008, Success = False\n",
      "Rollout - explore_agent - Reward = 1.0800000000000007, Success = False\n",
      "Rollout - explore_agent - Reward = 1.398000000000001, Success = False\n",
      "Rollout - explore_agent - Reward = 0.7100000000000004, Success = False\n",
      "Time spent stepping: 14.207250356674194\n",
      "Success Rate: 0.0%\n",
      "Repeating without training!\n",
      "value_loss: 0.003860281780362129\n",
      "policy_loss: -0.03805552050471306\n",
      "entropy: 1.1314526796340942\n",
      "old_approx_kl: 0.018310891464352608\n",
      "approx_kl: 0.016073832288384438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m      6\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplore_agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Machine Learning\\PokemonAI\\training.py:10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(policies, env, num_envs, target, trainer, num_steps, num_iterations, verbose)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(policies, env, num_envs, target, trainer: Trainer, num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#rollouts = generate_rollouts(policies, main_env, 'explore_agent', count=num_envs, max_steps=num_steps, verbose=True)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m         rollouts \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_rollouts_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         successes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39msuccess \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rollout \u001b[38;5;129;01min\u001b[39;00m rollouts[target]])\n\u001b[0;32m     13\u001b[0m         total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rollouts[target])\n",
      "File \u001b[1;32mg:\\Machine Learning\\PokemonAI\\rollout.py:247\u001b[0m, in \u001b[0;36mgenerate_rollouts_vec\u001b[1;34m(policies, env, target, count, collect, max_steps, verbose)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m#agent_obs = torch.tensor(np.array([agent_obs]), dtype=torch.float32)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 247\u001b[0m     action, log_prob, _, value \u001b[38;5;241m=\u001b[39m \u001b[43mpolicies\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(encoding[agent])):\n\u001b[0;32m    250\u001b[0m     e \u001b[38;5;241m=\u001b[39m encoding[agent][i]\n",
      "File \u001b[1;32mg:\\Machine Learning\\PokemonAI\\policy.py:44\u001b[0m, in \u001b[0;36mPolicy.get_action_and_value\u001b[1;34m(self, x, action)\u001b[0m\n\u001b[0;32m     42\u001b[0m probs \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, probs\u001b[38;5;241m.\u001b[39mlog_prob(action), probs\u001b[38;5;241m.\u001b[39mentropy(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(hidden)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\pokey\\lib\\site-packages\\torch\\distributions\\categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[0;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\pokey\\lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training import train\n",
    "\n",
    "#temp variables\n",
    "num_envs = 10\n",
    "num_steps = 400\n",
    "num_iterations = 100\n",
    "\n",
    "train(policies, vec_env, num_envs, 'explore_agent', ppo, num_steps=num_steps, num_iterations=num_iterations, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"hi\": 5, \"yo\": 4, \"tempd\": {\"what\": \"to\"}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tempd = {'what': 'to'}\n",
    "temp = {'hi': 5, 'yo': 4, 'tempd': tempd}\n",
    "json.dumps(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu = Emulator(sess_path, gb_path, instance_id='main', headless=True)\n",
    "emu.reset(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RED'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import read_string, bytes_to_string\n",
    "\n",
    "\n",
    "POKE1_TRAINER_NAME_ADDR = range(0xD273, 0xD27E)\n",
    "\n",
    "read_string(emu.pyboy, POKE1_TRAINER_NAME_ADDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RED'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes_to_string(bys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{79: ' ',\n",
       " 87: '#',\n",
       " 81: '*',\n",
       " 82: 'A1',\n",
       " 83: 'A2',\n",
       " 84: 'POKé',\n",
       " 85: '+',\n",
       " 88: '$',\n",
       " 117: '…',\n",
       " 127: ' ',\n",
       " 128: 'A',\n",
       " 129: 'B',\n",
       " 130: 'C',\n",
       " 131: 'D',\n",
       " 132: 'E',\n",
       " 133: 'F',\n",
       " 134: 'G',\n",
       " 135: 'H',\n",
       " 136: 'I',\n",
       " 137: 'J',\n",
       " 138: 'K',\n",
       " 139: 'L',\n",
       " 140: 'M',\n",
       " 141: 'N',\n",
       " 142: 'O',\n",
       " 143: 'P',\n",
       " 144: 'Q',\n",
       " 145: 'R',\n",
       " 146: 'S',\n",
       " 147: 'T',\n",
       " 148: 'U',\n",
       " 149: 'V',\n",
       " 150: 'W',\n",
       " 151: 'X',\n",
       " 152: 'Y',\n",
       " 153: 'Z',\n",
       " 154: '(',\n",
       " 155: ')',\n",
       " 156: ':',\n",
       " 157: ';',\n",
       " 158: '[',\n",
       " 159: ']',\n",
       " 160: 'a',\n",
       " 161: 'b',\n",
       " 162: 'c',\n",
       " 163: 'd',\n",
       " 164: 'e',\n",
       " 165: 'f',\n",
       " 166: 'g',\n",
       " 167: 'h',\n",
       " 168: 'i',\n",
       " 169: 'j',\n",
       " 170: 'k',\n",
       " 171: 'l',\n",
       " 172: 'm',\n",
       " 173: 'n',\n",
       " 174: 'o',\n",
       " 175: 'p',\n",
       " 176: 'q',\n",
       " 177: 'r',\n",
       " 178: 's',\n",
       " 179: 't',\n",
       " 180: 'u',\n",
       " 181: 'v',\n",
       " 182: 'w',\n",
       " 183: 'x',\n",
       " 184: 'y',\n",
       " 185: 'z',\n",
       " 186: 'é',\n",
       " 187: \"'d\",\n",
       " 188: \"'l\",\n",
       " 189: \"'s\",\n",
       " 190: \"'t\",\n",
       " 191: \"'v\",\n",
       " 224: \"'\",\n",
       " 225: 'PK',\n",
       " 226: 'MN',\n",
       " 227: '-',\n",
       " 228: \"'r\",\n",
       " 229: \"'m\",\n",
       " 230: '?',\n",
       " 231: '!',\n",
       " 232: '.',\n",
       " 237: '→',\n",
       " 238: '↓',\n",
       " 239: '♂',\n",
       " 240: '¥',\n",
       " 241: '×',\n",
       " 243: '/',\n",
       " 244: ',',\n",
       " 245: '♀',\n",
       " 246: '0',\n",
       " 247: '1',\n",
       " 248: '2',\n",
       " 249: '3',\n",
       " 250: '4',\n",
       " 251: '5',\n",
       " 252: '6',\n",
       " 253: '7',\n",
       " 254: '8',\n",
       " 255: '9'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_table.text_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[145, 132, 131, 80, 128, 146, 135, 80, 137, 128, 130]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x91'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(bys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7664861  pyboy.pyboy                    ERROR    State file not found: ./PokemonRed.gb.state\n",
      "7665041  pyboy.pyboy                    ERROR    State file not found: ./PokemonRed.gb.state\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m emu\u001b[38;5;241m.\u001b[39mreset(init_state)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43memu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Machine Learning\\PokemonAI\\emulator.py:107\u001b[0m, in \u001b[0;36mEmulator.run\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_freq\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyboy\u001b[38;5;241m.\u001b[39m_rendering(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyboy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m## TEMP\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emu = Emulator(sess_path, gb_path, instance_id='main', headless=False)\n",
    "emu.reset(init_state)\n",
    "while True:\n",
    "    emu.run(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ram_parsing import PARTY_ADDR, read_uint\n",
    "party = [emu.pyboy.get_memory_value(addr) for addr in PARTY_ADDR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[177, 255, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emu.pyboy.get_memory_value(0xD16B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import read_uint\n",
    "from ram_map import PARTY_POKEMON_ADDR_LIST, POKEMON_CURRENT_HP_OFFSET_RANGE\n",
    "\n",
    "read_uint(emu.pyboy, POKEMON_CURRENT_HP_OFFSET_RANGE, PARTY_POKEMON_ADDR_LIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pokemon ID': 22,\n",
       " 'Status': 0,\n",
       " 'Type 1': 21,\n",
       " 'Type 2': 21,\n",
       " 'Catch Rate': 45,\n",
       " 'Move 1': 33,\n",
       " 'Move 2': 39,\n",
       " 'Move 3': 0,\n",
       " 'Move 4': 0,\n",
       " 'Trainer ID': 25110,\n",
       " 'Experience': 202,\n",
       " 'HP EV': 45,\n",
       " 'Attack EV': 49,\n",
       " 'Defense EV': 49,\n",
       " 'Speed EV': 45,\n",
       " 'Special EV': 65,\n",
       " 'Attack/Defense IV': 135,\n",
       " 'Speed/Special IV': 146,\n",
       " 'PP Move 1': 35,\n",
       " 'PP Move 2': 30,\n",
       " 'PP Move 3': 0,\n",
       " 'PP Move 4': 0,\n",
       " 'Level': 6,\n",
       " 'Max HP': 22,\n",
       " 'Attack': 11,\n",
       " 'Defense': 13,\n",
       " 'Speed': 11,\n",
       " 'Special': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import parse_pokemon\n",
    "import ram_map\n",
    "from ram_map import PARTY_POKEMON_ADDR_LIST\n",
    "import importlib\n",
    "importlib.reload(ram_map)\n",
    "\n",
    "poke1 = parse_pokemon(emu.pyboy, PARTY_POKEMON_ADDR_LIST[0])\n",
    "poke1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POKEMON_EXP_OFFSET_RANGE = range(0xE, 0x11)\n",
    "from ram_parsing import read_uint\n",
    "\n",
    "read_uint(emu.pyboy, POKEMON_EXP_OFFSET_RANGE, PARTY_POKEMON_ADDR_LIST[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Count': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import parse_party\n",
    "from ram_map import PLAYER_PARTY_ADDR\n",
    "\n",
    "party = parse_party(emu.pyboy, 0xDA80, 20)\n",
    "party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pokemon ID': 177,\n",
       " 'Current HP': 22,\n",
       " 'Status': 0,\n",
       " 'Type 1': 21,\n",
       " 'Type 2': 21,\n",
       " 'Catch Rate': 45,\n",
       " 'Move 1': 33,\n",
       " 'Move 2': 39,\n",
       " 'Move 3': 0,\n",
       " 'Move 4': 0,\n",
       " 'Trainer ID': 25110,\n",
       " 'Experience': 202,\n",
       " 'HP EV': 45,\n",
       " 'Attack EV': 49,\n",
       " 'Defense EV': 49,\n",
       " 'Speed EV': 45,\n",
       " 'Special EV': 65,\n",
       " 'Attack/Defense IV': 135,\n",
       " 'Speed/Special IV': 146,\n",
       " 'PP Move 1': 35,\n",
       " 'PP Move 2': 30,\n",
       " 'PP Move 3': 0,\n",
       " 'PP Move 4': 0,\n",
       " 'Level': 6,\n",
       " 'Max HP': 22,\n",
       " 'Attack': 11,\n",
       " 'Defense': 13,\n",
       " 'Speed': 11,\n",
       " 'Special': 11,\n",
       " 'Pokemon Name': 'AAAAAAAAAA',\n",
       " 'Trainer Name': 'RED'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party['Pokemon 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Items': 1, 'Items': [('Potion', 1)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import parse_storage\n",
    "\n",
    "inv = parse_storage(emu.pyboy)\n",
    "inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emu.pyboy.get_memory_value(0xD322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ram_parsing import parse_money\n",
    "#975\n",
    "parse_money(emu.pyboy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emu.pyboy.get_memory_value(0xD349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Pallet Town',\n",
       " 1: 'Viridian City',\n",
       " 2: 'Pewter City',\n",
       " 3: 'Cerulean City',\n",
       " 4: 'Lavender Town',\n",
       " 5: 'Vermilion City',\n",
       " 6: 'Celadon City',\n",
       " 7: 'Fuchsia City',\n",
       " 8: 'Cinnabar Island',\n",
       " 9: 'Pokémon League',\n",
       " 10: 'Saffron City',\n",
       " 11: 'Unused Fly location',\n",
       " 12: 'Route 1',\n",
       " 13: 'Route 2',\n",
       " 14: 'Route 3',\n",
       " 15: 'Route 4',\n",
       " 16: 'Route 5',\n",
       " 17: 'Route 6',\n",
       " 18: 'Route 7',\n",
       " 19: 'Route 8',\n",
       " 20: 'Route 9',\n",
       " 21: 'Route 10',\n",
       " 22: 'Route 11',\n",
       " 23: 'Route 12',\n",
       " 24: 'Route 13',\n",
       " 25: 'Route 14',\n",
       " 26: 'Route 15',\n",
       " 27: 'Route 16',\n",
       " 28: 'Route 17',\n",
       " 29: 'Route 18',\n",
       " 30: 'Sea Route 19',\n",
       " 31: 'Sea Route 20',\n",
       " 32: 'Sea Route 21',\n",
       " 33: 'Route 22',\n",
       " 34: 'Route 23',\n",
       " 35: 'Route 24',\n",
       " 36: 'Route 25',\n",
       " 37: \"Red's house (first floor)\",\n",
       " 38: \"Red's house (second floor)\",\n",
       " 39: \"Blue's house\",\n",
       " 40: \"Professor Oak's Lab\",\n",
       " 41: 'Pokémon Center (Viridian City)',\n",
       " 42: 'Poké Mart (Viridian City)',\n",
       " 43: 'School (Viridian City)',\n",
       " 44: 'House 1 (Viridian City)',\n",
       " 45: 'Gym (Viridian City)',\n",
       " 46: \"Diglett's Cave (Route 2 entrance)\",\n",
       " 47: 'Gate (Viridian City/Pewter City) (Route 2)',\n",
       " 48: \"Oak's Aide House 1 (Route 2)\",\n",
       " 49: 'Gate (Route 2)',\n",
       " 50: 'Gate (Route 2/Viridian Forest) (Route 2)',\n",
       " 51: 'Viridian Forest',\n",
       " 52: 'Pewter Museum (floor 1)',\n",
       " 53: 'Pewter Museum (floor 2)',\n",
       " 54: 'Gym (Pewter City)',\n",
       " 55: 'House with disobedient Nidoran♂ (Pewter City)',\n",
       " 56: 'Poké Mart (Pewter City)',\n",
       " 57: 'House with two Trainers (Pewter City)',\n",
       " 58: 'Pokémon Center (Pewter City)',\n",
       " 59: 'Mt. Moon (Route 3 entrance)',\n",
       " 60: 'Mt. Moon',\n",
       " 61: 'Mt. Moon',\n",
       " 62: 'Invaded house (Cerulean City)',\n",
       " 63: 'Poliwhirl for Jynx trade house',\n",
       " 64: 'Pokémon Center (Cerulean City)',\n",
       " 65: 'Gym (Cerulean City)',\n",
       " 66: 'Bike Shop (Cerulean City)',\n",
       " 67: 'Poké Mart (Cerulean City)',\n",
       " 68: 'Pokémon Center (Route 4)',\n",
       " 69: 'Invaded house - alternative music (Cerulean City)',\n",
       " 70: 'Saffron City Gate (Route 5)',\n",
       " 71: 'Entrance to Underground Path (Route 5)',\n",
       " 72: 'Daycare Center (Route 5)',\n",
       " 73: 'Saffron City Gate (Route 6)',\n",
       " 74: 'Entrance to Underground Path (Route 6)',\n",
       " 75: 'Entrance to Underground Path (alternative music) (Route 6)',\n",
       " 76: 'Saffron City Gate (Route 7)',\n",
       " 77: 'Entrance to Underground Path (Route 7)',\n",
       " 78: 'Entrance to Underground Path (unused) (Route 7)',\n",
       " 79: 'Saffron City Gate (Route 8)',\n",
       " 80: 'Entrance to Underground Path (Route 8)',\n",
       " 81: 'Pokémon Center (Rock Tunnel)',\n",
       " 82: 'Rock Tunnel',\n",
       " 83: 'Power Plant',\n",
       " 84: 'Gate 1F (Route 11-Route 12)',\n",
       " 85: \"Diglett's Cave (Vermilion City entrance)\",\n",
       " 86: 'Gate 2F (Route 11-Route 12)',\n",
       " 87: 'Gate (Route 12-Route 13)',\n",
       " 88: 'Sea Cottage',\n",
       " 89: 'Pokémon Center (Vermilion City)',\n",
       " 90: 'Pokémon Fan Club (Vermilion City)',\n",
       " 91: 'Poké Mart (Vermilion City)',\n",
       " 92: 'Gym (Vermilion City)',\n",
       " 93: 'House with Pidgey (Vermilion City)',\n",
       " 94: 'Vermilion Harbor (Vermilion City)',\n",
       " 95: 'S.S. Anne 1F',\n",
       " 96: 'S.S. Anne 2F',\n",
       " 97: 'S.S. Anne 3F',\n",
       " 98: 'S.S. Anne B1F',\n",
       " 99: 'S.S. Anne (Deck)',\n",
       " 100: 'S.S. Anne (Kitchen)',\n",
       " 101: \"S.S. Anne (Captain's room)\",\n",
       " 102: \"S.S. Anne 1F (Gentleman's room)\",\n",
       " 103: \"S.S. Anne 2F (Gentleman's room)\",\n",
       " 104: \"S.S. Anne B1F (Sailor/Fisherman's room)\",\n",
       " 105: 'Unused (Victory Road)',\n",
       " 106: 'Unused (Victory Road)',\n",
       " 107: 'Unused (Victory Road)',\n",
       " 108: 'Victory Road (Route 23 entrance)',\n",
       " 109: 'Unused (Pokémon League)',\n",
       " 110: 'Unused (Pokémon League)',\n",
       " 111: 'Unused (Pokémon League)',\n",
       " 112: 'Unused (Pokémon League)',\n",
       " 113: \"Lance's Elite Four room\",\n",
       " 114: 'Unused (Pokémon League)',\n",
       " 115: 'Unused (Pokémon League)',\n",
       " 116: 'Unused (Pokémon League)',\n",
       " 117: 'Unused (Pokémon League)',\n",
       " 118: 'Hall of Fame',\n",
       " 119: 'Underground Path (Route 5-Route 6)',\n",
       " 120: \"Blue (Champion)'s room\",\n",
       " 121: 'Underground Path (Route 7-Route 8)',\n",
       " 122: 'Celadon Department Store 1F',\n",
       " 123: 'Celadon Department Store 2F',\n",
       " 124: 'Celadon Department Store 3F',\n",
       " 125: 'Celadon Department Store 4F',\n",
       " 126: 'Celadon Department Store Rooftop Square',\n",
       " 127: 'Celadon Department Store Lift',\n",
       " 128: 'Celadon Mansion 1F',\n",
       " 129: 'Celadon Mansion 2F',\n",
       " 130: 'Celadon Mansion 3F',\n",
       " 131: 'Celadon Mansion 4F',\n",
       " 132: 'Celadon Mansion 4F (Eevee building)',\n",
       " 133: 'Pokémon Center (Celadon City)',\n",
       " 134: 'Gym (Celadon City)',\n",
       " 135: 'Rocket Game Corner (Celadon City)',\n",
       " 136: 'Celadon Department Store 5F',\n",
       " 137: 'Prize corner (Celadon City)',\n",
       " 138: 'Restaurant (Celadon City)',\n",
       " 139: 'House with Team Rocket members (Celadon City)',\n",
       " 140: 'Hotel (Celadon City)',\n",
       " 141: 'Pokémon Center (Lavender Town)',\n",
       " 142: 'Pokémon Tower 1F',\n",
       " 143: 'Pokémon Tower 2F',\n",
       " 144: 'Pokémon Tower 3F',\n",
       " 145: 'Pokémon Tower 4F',\n",
       " 146: 'Pokémon Tower 5F',\n",
       " 147: 'Pokémon Tower 6F',\n",
       " 148: 'Pokémon Tower 7F',\n",
       " 149: \"Mr. Fuji's house (Lavender Town)\",\n",
       " 150: 'Poké Mart (Lavender Town)',\n",
       " 151: \"House with NPC discussing Cubone's mother\",\n",
       " 152: 'Poké Mart (Fuchsia City)',\n",
       " 153: 'House with NPCs discussing Bill (Fuchsia City)',\n",
       " 154: 'Pokémon Center (Fuchsia City)',\n",
       " 155: \"Warden's house (Fuchsia City)\",\n",
       " 156: 'Safari Zone gate (Fuchsia City)',\n",
       " 157: 'Gym (Fuchsia City)',\n",
       " 158: 'House with NPCs discussing Baoba (Fuchsia City)',\n",
       " 159: 'Seafoam Islands',\n",
       " 160: 'Seafoam Islands',\n",
       " 161: 'Seafoam Islands',\n",
       " 162: 'Seafoam Islands',\n",
       " 163: 'Vermilion City Fishing Brother',\n",
       " 164: 'Fuchsia City Fishing Brother',\n",
       " 165: 'Pokémon Mansion (1F)',\n",
       " 166: 'Gym (Cinnabar Island)',\n",
       " 167: 'Pokémon Lab (Cinnabar Island)',\n",
       " 168: 'Pokémon Lab - Trade room (Cinnabar Island)',\n",
       " 169: 'Pokémon Lab - Room with scientists (Cinnabar Island)',\n",
       " 170: 'Pokémon Lab - Fossil resurrection room (Cinnabar Island)',\n",
       " 171: 'Pokémon Center (Cinnabar Island)',\n",
       " 172: 'Poké Mart (Cinnabar Island)',\n",
       " 173: 'Poké Mart - alternative music (Cinnabar Island)',\n",
       " 174: 'Pokémon Center (Indigo Plateau)',\n",
       " 175: \"Copycat's house 1F (Saffron City)\",\n",
       " 176: \"Copycat's house 2F (Saffron City)\",\n",
       " 177: 'Fighting Dojo (Saffron City)',\n",
       " 178: 'Gym (Saffron City)',\n",
       " 179: 'House with Pidgey (Saffron City)',\n",
       " 180: 'Poké Mart (Saffron City)',\n",
       " 181: 'Silph Co. 1F',\n",
       " 182: 'Pokémon Center (Saffron City)',\n",
       " 183: \"Mr. Psychic's house (Saffron City)\",\n",
       " 184: 'Gate 1F (Route 15)',\n",
       " 185: 'Gate 2F (Route 15)',\n",
       " 186: 'Gate 1F (Cycling Road) (Route 16)',\n",
       " 187: 'Gate 2F (Cycling Road) (Route 16)',\n",
       " 188: 'Secret house (Cycling Road) (Route 16)',\n",
       " 189: 'Route 12 Fishing Brother',\n",
       " 190: 'Gate 1F (Route 18)',\n",
       " 191: 'Gate 2F (Route 18)',\n",
       " 192: 'Seafoam Islands',\n",
       " 193: 'Badges check gate (Route 22)',\n",
       " 194: 'Victory Road',\n",
       " 195: 'Gate 2F (Route 12)',\n",
       " 196: 'House with NPC and HM moves advice (Vermilion City)',\n",
       " 197: \"Diglett's Cave\",\n",
       " 198: 'Victory Road',\n",
       " 199: 'Team Rocket Hideout (B1F)',\n",
       " 200: 'Team Rocket Hideout (B2F)',\n",
       " 201: 'Team Rocket Hideout (B3F)',\n",
       " 202: 'Team Rocket Hideout (B4F)',\n",
       " 203: 'Team Rocket Hideout (Lift)',\n",
       " 204: 'Unused (Team Rocket Hideout)',\n",
       " 205: 'Unused (Team Rocket Hideout)',\n",
       " 206: 'Unused (Team Rocket Hideout)',\n",
       " 207: 'Silph Co. (2F)',\n",
       " 208: 'Silph Co. (3F)',\n",
       " 209: 'Silph Co. (4F)',\n",
       " 210: 'Silph Co. (5F)',\n",
       " 211: 'Silph Co. (6F)',\n",
       " 212: 'Silph Co. (7F)',\n",
       " 213: 'Silph Co. (8F)',\n",
       " 214: 'Pokémon Mansion (2F)',\n",
       " 215: 'Pokémon Mansion (3F)',\n",
       " 216: 'Pokémon Mansion (B1F)',\n",
       " 217: 'Safari Zone (Area 1)',\n",
       " 218: 'Safari Zone (Area 2)',\n",
       " 219: 'Safari Zone (Area 3)',\n",
       " 220: 'Safari Zone (Entrance)',\n",
       " 221: 'Safari Zone (Rest house 1)',\n",
       " 222: 'Safari Zone (Prize house)',\n",
       " 223: 'Safari Zone (Rest house 2)',\n",
       " 224: 'Safari Zone (Rest house 3)',\n",
       " 225: 'Safari Zone (Rest house 4)',\n",
       " 226: 'Cerulean Cave',\n",
       " 227: 'Cerulean Cave 1F',\n",
       " 228: 'Cerulean Cave B1F',\n",
       " 229: \"Name Rater's house (Lavender Town)\",\n",
       " 230: 'Cerulean City (Gym Badge man)',\n",
       " 231: 'Unused (Rock Tunnel)',\n",
       " 232: 'Rock Tunnel',\n",
       " 233: 'Silph Co. 9F',\n",
       " 234: 'Silph Co. 10F',\n",
       " 235: 'Silph Co. 11F',\n",
       " 236: 'Silph Co. Lift',\n",
       " 237: '(Invalid)',\n",
       " 238: '(Invalid)',\n",
       " 239: 'Cable Club Trade Center(*)',\n",
       " 240: 'Cable Club Colosseum(*)',\n",
       " 241: '(Invalid)',\n",
       " 242: '(Invalid)',\n",
       " 243: '(Invalid)',\n",
       " 244: '(Invalid)',\n",
       " 245: \"Lorelei's room\",\n",
       " 246: \"Bruno's room\",\n",
       " 247: \"Agatha's room\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from map_table import map_table\n",
    "\n",
    "map_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_state import save_state\n",
    "\n",
    "save_state(emu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu.run(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
